{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom Data Loader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "17ToRBMKswd1nHcFlvfCqiFcWJ7iPN0SQ",
      "authorship_tag": "ABX9TyORdL2ewAeDmuK2wUlEXZl/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HAMA-DL-dev/VML-internship/blob/master/Custom_Data_Loader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1qj8wX3vf_W"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import gzip\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDsHXnGkCb5G"
      },
      "source": [
        "def load_custom_mnist(image_root, label_root, bsize, shuffle):\n",
        "    dataset = MNISTCustomDataset(image_root, label_root)\n",
        "    loader = DataLoader(dataset, batch_size=bsize, shuffle=shuffle)\n",
        "\n",
        "    return loader"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di1CMs7qCfry"
      },
      "source": [
        "class MNISTCustomDataset(Dataset):\n",
        "    def __init__(self, image_data_root, label_data_root):\n",
        "        # image\n",
        "        self.image_data_root = image_data_root\n",
        "        self.image_magic_number = 0\n",
        "        self.num_images = 0\n",
        "        self.image_rows = 0\n",
        "        self.image_columns = 0\n",
        "        self.images = np.empty(0)\n",
        "        # label\n",
        "        self.label_data_root = label_data_root\n",
        "        self.label_magic_number = 0\n",
        "        self.num_labels = 0\n",
        "        self.labels = np.empty(0)\n",
        "\n",
        "        self.image_init_dataset()  # image data\n",
        "        self.label_init_dataset()  # label data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_images\n",
        "\n",
        "    # 여기서 이 메서드는 mnist 데이터에서 이미지를 읽는 역할.\n",
        "\n",
        "    def image_init_dataset(self):\n",
        "\n",
        "        # gzip 모듈을 통해 파일을 읽는, with 문법 기능을 수행.\n",
        "        # c언어의 fopen과 유사한 기능을 수행.\n",
        "        # 이때, 읽어야 하는 파일이 총 4개이기 때문에\n",
        "        # gzip 모듈을 쓰는 경우 코드를 간략하게 작성할 수 있음.\n",
        "        image_file = gzip.open(self.image_data_root, 'r')\n",
        "      \n",
        "        # 아래의 과정은 데이터셋을 위한 엔디안 byte order 변환 과정이다. (byte swapping)\n",
        "        # Intel CPU를 쓰는 경우 디폴트 값으로 low 엔디안 바이트 순서로 컴퓨팅이 되는데\n",
        "        # 이를통해 데이터를 읽을 때 문제가 없도록 할 수 있다.\n",
        "\n",
        "        r_type = np.dtype(np.int32).newbyteorder('>')\n",
        "      \n",
        "        # np.frombuffer( 바꾸고 싶은 bytes , dtype = <자료형>)\n",
        "        # read 함수는 파일의 내용 전체를 문자열로 돌려준다. \n",
        "        # 소괄호 안은 글자수. \n",
        "        self.image_magic_number = np.frombuffer(image_file.read(4), dtype=r_type)[0]\n",
        "        self.num_images = np.frombuffer(image_file.read(4), dtype=r_type)[0]\n",
        "        self.image_rows = np.frombuffer(image_file.read(4), dtype=r_type)[0]\n",
        "        self.image_columns = np.frombuffer(image_file.read(4), dtype=r_type)[0]\n",
        "                  \n",
        "        buffer = image_file.read(self.num_images * self.image_rows * self.image_columns)\n",
        "    \n",
        "        self.images = np.frombuffer(buffer, dtype=np.uint8).astype(np.float32)\n",
        "        \n",
        "        # 네트워크에 쓰기 위해 1차원 배열을 60000 x 784 크기로 reshape 한다. \n",
        "        self.images = np.reshape(self.images, (self.num_images, 784))\n",
        "        # This normalizes the data to be between 0 and 1.  255는 픽셀 값의 범위를 의미.\n",
        "        self.images = self.images / 255\n",
        "        \n",
        "        # 네트워크에 쓰기 위해 텐서변환\n",
        "        self.images = torch.tensor(self.images)\n",
        "          \n",
        "        # mnist 데이터에서 label을 불러오는 메서드.\n",
        "    def label_init_dataset(self):\n",
        "        label_file = gzip.open(self.label_data_root, 'r')\n",
        "\n",
        "        r_type = np.dtype(np.int32).newbyteorder('>')\n",
        "\n",
        "        self.label_magic_number = np.frombuffer(label_file.read(4), dtype=r_type).astype(np.int64)[0]\n",
        "        self.num_labels = np.frombuffer(label_file.read(4), dtype=r_type).astype(np.int64)[0]\n",
        "\n",
        "        buffer = label_file.read(self.num_labels)\n",
        "\n",
        "        self.labels = np.frombuffer(buffer, dtype=np.uint8)\n",
        "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9NhPCNUCieZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed978e5-87d6-4106-9082-c54711c2495c"
      },
      "source": [
        "trainloader = load_custom_mnist('/content/drive/My Drive/Colab Notebooks/train-images-idx3-ubyte.gz','/content/drive/My Drive/Colab Notebooks/train-labels-idx1-ubyte.gz', 10, True)\n",
        "testloader = load_custom_mnist('/content/drive/My Drive/Colab Notebooks/t10k-images-idx3-ubyte.gz','/content/drive/My Drive/Colab Notebooks/t10k-labels-idx1-ubyte.gz', 10, False)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:73: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxg7kuxeDqv8"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        \n",
        "        self.linear1 = nn.Linear(784, 100)\n",
        "        self.linear2 = nn.Linear(100, 50)\n",
        "       \n",
        "        self.linear3 = nn.Linear(50, 10)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.linear1(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "\n",
        "net=NeuralNet()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEJ2aVtADsfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350ee102-c6b2-4e16-daa3-639d53ce96bc"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "# 확률적 경사 하강법.\n",
        "optimizer=optim.SGD(net.parameters(),lr=0.001,momentum=0.9)\n",
        "\n",
        "for epoch in range(2):\n",
        "\n",
        "    running_loss=0.0\n",
        "    for i, data in enumerate(trainloader,0):\n",
        "        inputs,labels=data\n",
        "        inputs,labels=Variable(inputs),Variable(labels)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs=net(inputs)\n",
        "        loss=criterion(outputs,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss+=loss.data\n",
        "        if i % 2000==1999:\n",
        "\n",
        "            print(\"[%d %5d] loss: %.3f\"%(epoch+1,i+1,running_loss/2000))\n",
        "            running_loss=0.0"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1  2000] loss: 2.300\n",
            "[1  4000] loss: 2.280\n",
            "[1  6000] loss: 2.201\n",
            "[2  2000] loss: 1.866\n",
            "[2  4000] loss: 1.384\n",
            "[2  6000] loss: 1.069\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}